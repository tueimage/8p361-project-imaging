{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prospective-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable overly verbose tensorflow logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}   \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, Flatten, Conv2D, MaxPool2D, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "# unused for now, to be used for ROC analysis\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "allow_growth = True\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-buying",
   "metadata": {},
   "source": [
    "# Initialize the MobileNetV2 model for fine-tuning on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electoral-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 96, 96, 3)]       0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_96 (Functio (None, 3, 3, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "\n",
    "input = Input(input_shape)\n",
    "\n",
    "# get the pretrained model, cut out the top layer\n",
    "pretrained = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "# if the pretrained model it to be used as a feature extractor, and not for\n",
    "# fine-tuning, the weights of the model can be frozen in the following way\n",
    "# for layer in pretrained.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "output = pretrained(input)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input, output)\n",
    "\n",
    "# note the lower lr compared to the cnn example\n",
    "model.compile(SGD(lr=0.001, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print a summary of the model on screen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-delivery",
   "metadata": {},
   "source": [
    "# Get the datagenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "running-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\t \n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sudden-uganda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators(r'C:\\Users\\20153761\\Documents\\TUe\\4e jaar\\3e kwartiel\\BIA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-purpose",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "multiple-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20153761\\Anaconda3\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 62s 243ms/step - loss: 0.5850 - accuracy: 0.7359 - val_loss: 2.5186 - val_accuracy: 0.4787\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.51858, saving model to transfer_4.2_ImageNet_model_weights.hdf5\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 51s 224ms/step - loss: 0.3597 - accuracy: 0.8453 - val_loss: 3.2356 - val_accuracy: 0.5063\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.51858\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 50s 224ms/step - loss: 0.3075 - accuracy: 0.8704 - val_loss: 1.5066 - val_accuracy: 0.6463\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.51858 to 1.50656, saving model to transfer_4.2_ImageNet_model_weights.hdf5\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.2854 - accuracy: 0.8854 - val_loss: 0.7661 - val_accuracy: 0.7425\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.50656 to 0.76613, saving model to transfer_4.2_ImageNet_model_weights.hdf5\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 47s 208ms/step - loss: 0.2659 - accuracy: 0.8881 - val_loss: 0.9580 - val_accuracy: 0.6587\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.76613\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 53s 234ms/step - loss: 0.2589 - accuracy: 0.8979 - val_loss: 0.2973 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.76613 to 0.29725, saving model to transfer_4.2_ImageNet_model_weights.hdf5\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.2366 - accuracy: 0.9048 - val_loss: 0.3937 - val_accuracy: 0.8687\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.29725\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 54s 241ms/step - loss: 0.2155 - accuracy: 0.9203 - val_loss: 0.3661 - val_accuracy: 0.8537\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.29725\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 50s 223ms/step - loss: 0.2335 - accuracy: 0.9140 - val_loss: 0.2255 - val_accuracy: 0.9087\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.29725 to 0.22551, saving model to transfer_4.2_ImageNet_model_weights.hdf5\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 52s 231ms/step - loss: 0.1980 - accuracy: 0.9221 - val_loss: 0.2820 - val_accuracy: 0.8925\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22551\n"
     ]
    }
   ],
   "source": [
    "# save the model and weights\n",
    "model_name = 'transfer_4.2_ImageNet_model'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model, note that we define \"mini-epochs\"\n",
    "train_steps = train_gen.n//train_gen.batch_size//20\n",
    "val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "# since the model is trained for only 10 \"mini-epochs\", i.e. half of the data is\n",
    "# not used during training\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-choir",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-baking",
   "metadata": {},
   "source": [
    "### View loss graph\n",
    "````bash\n",
    "activate 8p361\n",
    "cd 'path/where/logs/are'\n",
    "tensorboard --logdir logs\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-preservation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
